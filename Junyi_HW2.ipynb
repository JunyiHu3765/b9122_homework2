{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import requests\n",
    "from lxml import etree\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and extract all hyperlinks in the url page\n",
    "def find_hrefs(url, hrefs, headers, base_url, task):\n",
    "    '''\n",
    "    url: request site\n",
    "    \n",
    "    hrefs: all previous hyperlinks\n",
    "    headers: HTTP headers\n",
    "    '''\n",
    "    # request the url\n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    selector = etree.HTML(response.text)\n",
    "    hrefs_temp = selector.xpath('//@href')\n",
    "    if task == \"UN\":\n",
    "        # extract only hyperlink starting with \"/en\"\n",
    "        hrefs_temp = [base_url + \"/\" + s.lstrip('/en')  for s in hrefs_temp if s.startswith('/en') and not s.endswith('.xml')]\n",
    "    elif task == \"EU\":\n",
    "        hrefs_temp = [base_url + \"/\" + s.lstrip(base_url)  for s in hrefs_temp if \"press-room\" in s and not s.endswith('.xml')]\n",
    "    hrefs_temp = hrefs + hrefs_temp\n",
    "    # every href in the output should be unique\n",
    "    hrefs_unique = list(set(hrefs_temp))\n",
    "    hrefs_unique.sort(key=hrefs_temp.index)\n",
    "    return (hrefs_unique)\n",
    "\n",
    "# Whether page satisfies certian conditions\n",
    "def type_keyword_judge(url, headers, url_type, type_xpath, keyword, keyword_xpath, task, n_release):\n",
    "    '''\n",
    "    url: request site\n",
    "    url_type: whether page belongs to the type\n",
    "    type_xpath: type xpath\n",
    "    keyword: whether page contains the keyword\n",
    "    keyword_xpath: keyword xpath\n",
    "    task: UN or EU task\n",
    "    n_release: number of press release\n",
    "    '''\n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    selector = etree.HTML(response.text)\n",
    "    # type consistency\n",
    "    anchor_tag = url_type in selector.xpath(type_xpath)\n",
    "    # keyword consistency\n",
    "    if anchor_tag:\n",
    "        text = selector.xpath(keyword_xpath)\n",
    "        # remove all punctuation marks\n",
    "        long_text_lower = re.sub(r'[^\\w\\s]', '', \" \".join(text))\n",
    "        # replace other spaces (like newline) with x20 space \n",
    "        long_text_lower = re.sub(r'[^\\w]', ' ', long_text_lower)\n",
    "        # whether page contains the keyword\n",
    "        if (\" \" + keyword + \" \") in (\" \" + long_text_lower + \" \"):\n",
    "            save_page(selector, task, n_release)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# One round scraping all hyperlinks containing in the current urls list\n",
    "def single_crawler(output, base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task, n_release):\n",
    "    '''\n",
    "    output is a dic containing three elements:\n",
    "        hrefs: a list consisting of all urls to request\n",
    "        index: index of the previous hrefs list denoting all pages satisfying certain conditions\n",
    "        len: where to start requesting in the hrefs list\n",
    "    '''\n",
    "    temp_hrefs = output[\"hrefs\"].copy()\n",
    "    index = output[\"index\"]\n",
    "    # start requesting from len-th element in the href list\n",
    "    for href in output[\"hrefs\"][output[\"len\"]:]:\n",
    "        # extract and store hyperlinks\n",
    "        temp_hrefs =  find_hrefs(href, temp_hrefs, headers, base_url, task)\n",
    "        # if having found one, store its index in the hrefs list\n",
    "        if type_keyword_judge(href, headers, url_type, type_xpath, keyword, keyword_xpath, task, n_release) == 1:\n",
    "            # a reminder\n",
    "            print(href, \"saved as\", task, \"task\", n_release, \"th file\")\n",
    "            n_release += 1\n",
    "            # url index\n",
    "            index.append(output[\"hrefs\"].index(href))\n",
    "    return {\"hrefs\": temp_hrefs, \"index\": index, \"len\": len(output[\"hrefs\"])}            \n",
    "\n",
    "# Recursively adopt the previous search\n",
    "def recursively_crawlers(base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task):\n",
    "    n_release = 1\n",
    "    output = {\"hrefs\": find_hrefs(base_url, [], headers, base_url, task),\n",
    "              \"index\": [],\n",
    "              \"len\": 0}\n",
    "    while n_release < 10:\n",
    "        output = single_crawler(output, base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task, n_release)\n",
    "        n_release = len(output[\"index\"]) + 1\n",
    "    return output\n",
    "\n",
    "# Save html code as txt file\n",
    "def save_page(selector, task, n_release):\n",
    "    html_as_string = etree.tostring(selector, pretty_print = True, encoding = \"utf-8\").decode()\n",
    "    if task == \"UN\":\n",
    "        task = 1\n",
    "    elif task == \"EU\":\n",
    "        task = 2\n",
    "    with open(str(task) + \"_\" + str(n_release) + \".txt\", 'w') as f: \n",
    "        f.write(html_as_string)\n",
    "        f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://press.un.org/en/2023/sgsm21982.doc.htm saved as UN task 1 th file\n",
      "https://press.un.org/en/2023/sgsm21980.doc.htm saved as UN task 2 th file\n",
      "https://press.un.org/en/2023/sgsm21978.doc.htm saved as UN task 3 th file\n",
      "https://press.un.org/en/2023/sgsm21947.doc.htm saved as UN task 4 th file\n",
      "https://press.un.org/en/2023/dsgsm1874.doc.htm saved as UN task 5 th file\n",
      "https://press.un.org/en/2023/sgsm21952.doc.htm saved as UN task 6 th file\n",
      "https://press.un.org/en/2023/sgsm21876.doc.htm saved as UN task 7 th file\n",
      "https://press.un.org/en/2023/sgsm21852.doc.htm saved as UN task 8 th file\n",
      "https://press.un.org/en/2023/sgsm21806.doc.htm saved as UN task 9 th file\n",
      "https://press.un.org/en/2023/dsgsm1848.doc.htm saved as UN task 10 th file\n",
      "https://press.un.org/en/2023/sgsm21765.doc.htm saved as UN task 11 th file\n",
      "https://press.un.org/en/2023/sgsm21767.doc.htm saved as UN task 12 th file\n",
      "https://press.un.org/en/2023/sgsm21723.doc.htm saved as UN task 13 th file\n",
      "https://press.un.org/en/2023/dsgsm1835.doc.htm saved as UN task 14 th file\n",
      "https://press.un.org/en/2023/sgsm21713.doc.htm saved as UN task 15 th file\n",
      "https://press.un.org/en/2023/dsgsm1837.doc.htm saved as UN task 16 th file\n",
      "https://press.un.org/en/2023/dsgsm1833.doc.htm saved as UN task 17 th file\n",
      "https://press.un.org/en/2023/sgsm21709.doc.htm saved as UN task 18 th file\n",
      "https://press.un.org/en/2023/sgsm21706.doc.htm saved as UN task 19 th file\n",
      "https://press.un.org/en/2021/dev3440.doc.htm saved as UN task 20 th file\n",
      "https://press.un.org/en/2021/dev3439.doc.htm saved as UN task 21 th file\n"
     ]
    }
   ],
   "source": [
    "# Scrape the UN press room\n",
    "base_url = \"https://press.un.org/en\"\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url_type = \"Press Release\"\n",
    "type_xpath = '//a[@href = \"/en/press-release\" and @hreflang = \"en\"]/text()'\n",
    "keyword = \"crisis\"\n",
    "keyword_xpath = '//h1[@class = \"page-header\"]/text() | //div[@class = \"field field--name-body field--type-text-with-summary field--label-hidden field__item\"]/p//text()'\n",
    "\n",
    "output = recursively_crawlers(base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task = \"UN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan saved as EU task 1 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06130/parliament-argues-for-a-top-up-to-multi-annual-budget-for-crisis-response saved as EU task 2 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04923/reduce-demand-and-protect-people-in-prostitution-say-meps saved as EU task 3 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04918/svietlana-tsikhanouskaya-to-meps-support-belarusians-european-aspirations saved as EU task 4 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04908/meps-vote-to-strengthen-eu-defence-industry-through-common-procurement saved as EU task 5 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02427/covid-19-parliament-adopts-roadmap-to-better-prepare-for-future-health-crises saved as EU task 6 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings saved as EU task 7 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02418/semiconductors-meps-adopt-legislation-to-boost-eu-chips-industry saved as EU task 8 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230706IPR02317/ep-today saved as EU task 9 th file\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230706IPR02316/ep-today saved as EU task 10 th file\n"
     ]
    }
   ],
   "source": [
    "# Scrape the EU press room\n",
    "base_url = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url_type = \"Plenary session\"\n",
    "type_xpath = '//span[@class = \"ep_name\"]/text()'\n",
    "keyword = \"crisis\"\n",
    "keyword_xpath = '//*[@id=\"website-body\"]/div[1]/div/div[2]/div/div/h1/div/span[1]/text() | //*[@id=\"website-body\"]/div[2]/div/div[3]/div/div/div[1]/div/div/div/ul//span[@class = \"ep_name\"]/text() | //*[@id=\"website-body\"]/div[2]/div/div[3]/div/div/div[2]/div/div/p/text() | //p[@class = \"ep-wysiwig_paragraph\"]/text()'\n",
    "\n",
    "# Need specify urls instead of crawling automatically\n",
    "n_articles = 1\n",
    "n_page = 0\n",
    "while n_articles < 10:\n",
    "    output = {\"hrefs\": find_hrefs(base_url + \"/page/\" + str(n_page), [], headers, base_url, \"EU\"),\n",
    "              \"index\": [],\n",
    "              \"len\": 0}\n",
    "    output = single_crawler(output, base_url, headers, url_type, type_xpath, keyword, keyword_xpath, \"EU\", n_articles)\n",
    "    n_articles += len(output[\"index\"])\n",
    "    n_page += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
