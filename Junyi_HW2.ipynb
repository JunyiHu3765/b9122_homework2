{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import re\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and extract all hyperlinks in the url page\n",
    "def find_hrefs(url, hrefs, headers, base_url, task):\n",
    "    '''\n",
    "    url: request site\n",
    "    \n",
    "    hrefs: all previous hyperlinks\n",
    "    headers: HTTP headers\n",
    "    '''\n",
    "    # request the url\n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    selector = etree.HTML(response.text)\n",
    "    hrefs_temp = selector.xpath('//@href')\n",
    "    if task == \"UN\":\n",
    "        # extract only hyperlink starting with \"/en\"\n",
    "        hrefs_temp = [base_url + \"/\" + s.lstrip('/en')  for s in hrefs_temp if s.startswith('/en') and not s.endswith('.xml')]\n",
    "    elif task == \"EU\":\n",
    "        hrefs_temp = [base_url + \"/\" + s.lstrip(base_url)  for s in hrefs_temp if \"press-room\" in s and not s.endswith('.xml')]\n",
    "    hrefs_temp = hrefs + hrefs_temp\n",
    "    # every href in the output should be unique\n",
    "    hrefs_unique = list(set(hrefs_temp))\n",
    "    hrefs_unique.sort(key=hrefs_temp.index)\n",
    "    return (hrefs_unique)\n",
    "\n",
    "# Whether page satisfies certian conditions\n",
    "def type_keyword_judge(url, headers, url_type, type_xpath, keyword, keyword_xpath):\n",
    "    '''\n",
    "    url: request site\n",
    "    url_type: whether page belongs to the type\n",
    "    type_xpath: type xpath\n",
    "    keyword: whether page contains the keyword\n",
    "    keyword_xpath: keyword xpath\n",
    "    '''\n",
    "    response = requests.get(url = url, headers = headers)\n",
    "    selector = etree.HTML(response.text)\n",
    "    # type consistency\n",
    "    anchor_tag = url_type in selector.xpath(type_xpath)\n",
    "    # keyword consistency\n",
    "    if anchor_tag:\n",
    "        text = selector.xpath(keyword_xpath)\n",
    "        # remove all punctuation marks\n",
    "        long_text_lower = re.sub(r'[^\\w\\s]', '', \" \".join(text))\n",
    "        # replace other spaces (like newline) with x20 space \n",
    "        long_text_lower = re.sub(r'[^\\w]', ' ', long_text_lower)\n",
    "        # whether page contains the keyword\n",
    "        if (\" \" + keyword + \" \") in (\" \" + long_text_lower + \" \"):\n",
    "            save_page(text, long_text_lower)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# One round scraping all hyperlinks containing in the current urls list\n",
    "def single_crawler(output, base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task):\n",
    "    '''\n",
    "    output is a dic containing three elements:\n",
    "        hrefs: a list consisting of all urls to request\n",
    "        index: index of the previous hrefs list denoting all pages satisfying certain conditions\n",
    "        len: where to start requesting in the hrefs list\n",
    "    '''\n",
    "    temp_hrefs = output[\"hrefs\"].copy()\n",
    "    index = output[\"index\"]\n",
    "    # start requesting from len-th element in the href list\n",
    "    for href in output[\"hrefs\"][output[\"len\"]:]:\n",
    "        # extract and store hyperlinks\n",
    "        temp_hrefs =  find_hrefs(href, temp_hrefs, headers, base_url, task)\n",
    "        # if having found one, store its index in the hrefs list\n",
    "        if type_keyword_judge(href, headers, url_type, type_xpath, keyword, keyword_xpath) == 1:\n",
    "            # a reminder\n",
    "            print(href, \"saved. index:\", output[\"hrefs\"].index(href), )\n",
    "            # url index\n",
    "            index.append(output[\"hrefs\"].index(href))\n",
    "    return {\"hrefs\": temp_hrefs, \"index\": index, \"len\": len(output[\"hrefs\"])}            \n",
    "\n",
    "# Recursively adopt the previous search\n",
    "def recursively_crawlers(base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task):\n",
    "    output = {\"hrefs\": find_hrefs(base_url, [], headers, base_url, task),\n",
    "              \"index\": [],\n",
    "              \"len\": 0}\n",
    "    while len(output[\"index\"]) < 10:\n",
    "        output = single_crawler(output, base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task)\n",
    "    return output\n",
    "\n",
    "def save_page(text, long_text):\n",
    "    path = os.path.join(\"articles\", re.sub(r'[^\\w\\s]', '', text[0]) + \".txt\")\n",
    "    with open(path, 'w') as f: \n",
    "        f.write(long_text) \n",
    "        f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://press.un.org/en/2023/sgsm21982.doc.htm saved. index: 23\n",
      "https://press.un.org/en/2023/sgsm21980.doc.htm saved. index: 25\n",
      "https://press.un.org/en/2023/sgsm21978.doc.htm saved. index: 28\n",
      "https://press.un.org/en/2023/sgsm21947.doc.htm saved. index: 85\n",
      "https://press.un.org/en/2023/dsgsm1874.doc.htm saved. index: 86\n",
      "https://press.un.org/en/2023/sgsm21952.doc.htm saved. index: 103\n",
      "https://press.un.org/en/2023/sgsm21876.doc.htm saved. index: 124\n",
      "https://press.un.org/en/2023/sgsm21852.doc.htm saved. index: 135\n",
      "https://press.un.org/en/2023/sgsm21806.doc.htm saved. index: 137\n",
      "https://press.un.org/en/2023/dsgsm1848.doc.htm saved. index: 138\n",
      "https://press.un.org/en/2023/sgsm21765.doc.htm saved. index: 140\n",
      "https://press.un.org/en/2023/sgsm21767.doc.htm saved. index: 141\n",
      "https://press.un.org/en/2023/sgsm21723.doc.htm saved. index: 186\n",
      "https://press.un.org/en/2023/dsgsm1835.doc.htm saved. index: 191\n",
      "https://press.un.org/en/2023/sgsm21713.doc.htm saved. index: 196\n",
      "https://press.un.org/en/2023/dsgsm1837.doc.htm saved. index: 200\n",
      "https://press.un.org/en/2023/dsgsm1833.doc.htm saved. index: 201\n",
      "https://press.un.org/en/2023/sgsm21709.doc.htm saved. index: 215\n",
      "https://press.un.org/en/2023/sgsm21706.doc.htm saved. index: 216\n",
      "https://press.un.org/en/2021/dev3440.doc.htm saved. index: 222\n",
      "https://press.un.org/en/2021/dev3439.doc.htm saved. index: 223\n"
     ]
    }
   ],
   "source": [
    "# Scrape the UN press room\n",
    "base_url = \"https://press.un.org/en\"\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url_type = \"Press Release\"\n",
    "type_xpath = '//a[@href = \"/en/press-release\" and @hreflang = \"en\"]/text()'\n",
    "keyword = \"crisis\"\n",
    "keyword_xpath = '//h1[@class = \"page-header\"]/text() | //div[@class = \"field field--name-body field--type-text-with-summary field--label-hidden field__item\"]/p//text()'\n",
    "\n",
    "output = recursively_crawlers(base_url, headers, url_type, type_xpath, keyword, keyword_xpath, task = \"UN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan saved. index: 2\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06130/parliament-argues-for-a-top-up-to-multi-annual-budget-for-crisis-response saved. index: 1\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04923/reduce-demand-and-protect-people-in-prostitution-say-meps saved. index: 11\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04918/svietlana-tsikhanouskaya-to-meps-support-belarusians-european-aspirations saved. index: 2\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04908/meps-vote-to-strengthen-eu-defence-industry-through-common-procurement saved. index: 10\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02427/covid-19-parliament-adopts-roadmap-to-better-prepare-for-future-health-crises saved. index: 11\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings saved. index: 4\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02418/semiconductors-meps-adopt-legislation-to-boost-eu-chips-industry saved. index: 7\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230706IPR02317/ep-today saved. index: 11\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230706IPR02316/ep-today saved. index: 14\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230609IPR96801/meps-want-to-create-a-european-day-for-the-victims-of-the-global-climate-crises saved. index: 7\n"
     ]
    }
   ],
   "source": [
    "# Scrape the EU press room\n",
    "base_url = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url_type = \"Plenary session\"\n",
    "type_xpath = '//span[@class = \"ep_name\"]/text()'\n",
    "keyword = \"crisis\"\n",
    "keyword_xpath = '//*[@id=\"website-body\"]/div[1]/div/div[2]/div/div/h1/div/span[1]/text() | //*[@id=\"website-body\"]/div[2]/div/div[3]/div/div/div[1]/div/div/div/ul//span[@class = \"ep_name\"]/text() | //*[@id=\"website-body\"]/div[2]/div/div[3]/div/div/div[2]/div/div/p/text() | //p[@class = \"ep-wysiwig_paragraph\"]/text()'\n",
    "\n",
    "# Need specify urls instead of crawling automatically\n",
    "n_articles = 0\n",
    "n_page = 0\n",
    "while n_articles <= 10:\n",
    "    output = {\"hrefs\": find_hrefs(base_url + \"/page/\" + str(n_page), [], headers, base_url, \"EU\"),\n",
    "              \"index\": [],\n",
    "              \"len\": 0}\n",
    "    output = single_crawler(output, base_url, headers, url_type, type_xpath, keyword, keyword_xpath, \"EU\")\n",
    "    n_articles += len(output[\"index\"])\n",
    "    n_page += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
